{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HPE Ezmeral Container Platform ML Ops - Lab 2\n",
        "## Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### About the Model\n",
        "This tutorial uses a XGboost based python model written to classify a person's income as being either less than or equal to, or more than, $ 50,000 \n",
        "\n",
        "### Setup correct directory paths\n",
        "Please, make sure all references to <userID> below in the directory paths for models and data subfolders match your own userID. For example, replace data/UCI_Income/\\<userID\\> with data/UCI_Income/student$$I\n",
        "\n",
        "### Test Connection to Training Cluster\n",
        "\n",
        "First we'll test that the training cluster is indeed functioning properly. \n",
        "\n",
        "<b>%attachments</b> is a line magic command that is special to the HPE Ezmeral ML Ops Jupyter notebooks. During the notebook creation step, we have the option to attach the notebook to a training cluster. This line magic command will output a table with the name(s) of the training cluster(s) available for us to use. Sometimes, tenant admin may have created multiple training clusters for different projects depending on the needs of the model or size of data, e.g. some with GPU nodes, while others with CPUs only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%attachments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "userID=\"student$$I\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To utilize the training cluster, we will need grab the name of the training cluster you want to use and feed it into another custom line magic command. \n",
        "\n",
        "In this lab, we are going to use the **pythonmldl** training cluster that is the common cluster for all training jobs. \n",
        "\n",
        "The Jupyter notebook will then send the contents of the cell to be executed on the training cluster. Any work that you have done in this notebook will be not be propogated to the training cluster. Therefore you will need to import the libraries and re-write any code you need to be excuted on the training cluster. \n",
        "\n",
        "The example cell below will execute a print statement on the training cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%pythonmldl\n",
        "\n",
        "print('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training cluster will send back a unique log url to this particular user and notebook. You can use this URL with another custom line magic command to track the status of the job in real time. \n",
        "\n",
        "Copy the URL output from the previous cell and paste it into the cell below where it says \"your_url_here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%logs --url http://hpecp-21.cplocal:10001/history/16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now we can start coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=1.5)\n",
        "\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we are definining a function to return us the path to the Project Repository. Every cluster in the ML Ops tenant has read and write privileges to the Project Repository. There are two different ways to access the project repo:\n",
        "1. You can copy the direct path from the HPE Ezmeral CP UI \n",
        "2. You can use the bdvcli command as seen below in the function to grab the path\n",
        "\n",
        "bdvcli is a custom command line tool used to obtain information about HEPCP. See [bdvcli documentation](https://github.com/bluedatainc/solutions/blob/master/bdvcli_commands/bdvcli_commands.md) for list of commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ProjectRepo(path):\n",
        "   ProjectRepo = os.popen('bdvcli --get cluster.project_repo').read().rstrip()\n",
        "   return ProjectRepo + '/' + path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strpath= \"data/UCI_Income/\" + userID + \"/adult_data.csv\"\n",
        "train_file = ProjectRepo(strpath)\n",
        "train_set = pd.read_csv(train_file, header=None)\n",
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strpath= \"data/UCI_Income/\" + userID + \"/adult_test.csv\"\n",
        "test_file = ProjectRepo(strpath)\n",
        "test_set = pd.read_csv(test_file, skiprows=1, header=None)\n",
        "test_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Findings\n",
        "1. No column headers in the data (can fix using dataset description from website)\n",
        "2. Some \"?\" in test data \n",
        "3. Target values differ in train and test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Fix column headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \n",
        "              'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "             'wage_class']\n",
        "train_set.columns = col_labels\n",
        "test_set.columns = col_labels\n",
        "train_set.info()\n",
        "test_set.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Clean up ? in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set.replace(' ?', np.nan).dropna().shape\n",
        "test_set.replace(' ?', np.nan).dropna().shape\n",
        "# removing rows with \"?\" from our dataframes \n",
        "train_no_missing = train_set.replace(' ?', np.nan).dropna()\n",
        "test_no_missing = test_set.replace(' ?', np.nan).dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Fix targets (remove the extra periods from '<=50K.' to '<=50K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_no_missing['wage_class'] = test_no_missing.wage_class.replace({' <=50K.' : ' <=50K', ' >50K.' : ' >50K'})\n",
        "test_no_missing.wage_class.unique()\n",
        "train_no_missing.wage_class.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying ordinal encoding to categoricals\n",
        "- ordinal encoding: convert string labels to integer values 1 through k. First unique value in column becomes 1, the second becomes 2, the third becomes 3, adn so on\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#combine the datasets together first\n",
        "combined_set = pd.concat([train_no_missing, test_no_missing], axis=0)\n",
        "combined_set.info()\n",
        "#Visualizations after initial cleaning of dataset \n",
        "group = combined_set.groupby('wage_class')\n",
        "group\n",
        "#encode non-numerical features into numeric values using pandas Cateogrical codes \n",
        "#and generating categorical codes mapping into dictionary\n",
        "cat_codes = {}\n",
        "for feature in combined_set.columns: \n",
        "    if combined_set[feature].dtype == 'object':\n",
        "        #workclass : { occupation : number }\n",
        "        temp_dict = {}\n",
        "        feature_codes = list(pd.Categorical(combined_set[feature]).codes)\n",
        "        feature_list = list(combined_set[feature])\n",
        "        for i in range(len(feature_codes)):\n",
        "            temp_dict[feature_list[i].strip()] = int(feature_codes[i])\n",
        "            if len(temp_dict) > len(feature_list):\n",
        "                break\n",
        "        cat_codes[feature] = temp_dict\n",
        "        combined_set[feature] = pd.Categorical(combined_set[feature]).codes\n",
        "combined_set.info()\n",
        "# saving encoding to json file to be used for scoring script\n",
        "strpath= \"data/UCI_Income/\" + userID + \"/encoding.json\"\n",
        "json_file = ProjectRepo(strpath)\n",
        "with open(json_file, 'w') as file:\n",
        "    json.dump(cat_codes, file)\n",
        "    #split combined set back into test/train split \n",
        "final_train = combined_set[:train_no_missing.shape[0]] \n",
        "final_test = combined_set[train_no_missing.shape[0]:]\n",
        "strpath= \"data/UCI_Income/\" + userID + \"/adult_train_cleaned.csv\"\n",
        "final_train.to_csv(ProjectRepo(strpath))\n",
        "strpath= \"data/UCI_Income/\" + userID + \"/adult_test_cleaned.csv\"\n",
        "final_test.to_csv(ProjectRepo(strpath))\n",
        "#extracting target values from our test and train sets \n",
        "y_train = final_train.pop('wage_class')\n",
        "y_test = final_test.pop('wage_class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Development\n",
        "\n",
        "### First model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
        "ind_params = {'learning_Rate': 0.1, 'n_estimators': 1000, 'seed': 0, 'subsample' : 0.8, 'colsample_bytree': 0.8, \n",
        "              'objective': 'binary:logistic'}\n",
        "\n",
        "#optimizing for accuracy, GBM = gradient boost model\n",
        "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
        "                             cv_params, \n",
        "                             scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
        "optimized_GBM.fit(final_train, y_train)\n",
        "optimized_GBM.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Second model\n",
        "Tuning other hyperparameters in an attempt to achieve higher mean accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7, 0.8, 0.9]}\n",
        "ind_params = {'n_estimators': 1000, 'seed': 0, 'colsample_bytree': 0.8, 'objective': 'binary:logistic', \n",
        "              'max_depth': 3, 'min_child_weight': 1}\n",
        "                    \n",
        "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
        "                             cv_params, \n",
        "                             scoring = 'accuracy', cv=5, n_jobs=-1)\n",
        "optimized_GBM.fit(final_train, y_train)\n",
        "optimized_GBM.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Third model\n",
        "Utilize XGBoost's built-in cv which allows early stopping to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgdmat = xgb.DMatrix(final_train, y_train)\n",
        "our_params = {'eta': 0.1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'binary:logistic',\n",
        "              'max_depth': 3, 'min_child_weight': 1}\n",
        "\n",
        "cv_xgb = xgb.cv(params=our_params, dtrain=xgdmat, num_boost_round=3000, metrics=['error'],\n",
        "                early_stopping_rounds=100)\n",
        "print('Best iteration:', len(cv_xgb))\n",
        "cv_xgb.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
        "             'objective': 'binary:logistic', 'max_depth':3, 'min_child_weight':1} \n",
        "\n",
        "final_gb = xgb.train(our_params, xgdmat, num_boost_round = 326)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot feature importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb.plot_importance(final_gb)\n",
        "importances = final_gb.get_fscore()\n",
        "importances\n",
        "importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})\n",
        "importance_frame.sort_values(by = 'Importance', inplace=True)\n",
        "importance_frame.plot(kind='barh', x='Feature', figsize=(8,8), color='green')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build model remotely on a distributed Python training cluster\n",
        "\n",
        "This training job combines all the cells we've worked on preeviously and form one large cell. At the end, we will save the model into the Project Repository. \n",
        "\n",
        "Make sure you fill your <b>training cluster name</b> in the line magic! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%pythonmldl\n",
        "userID=\"student$$I\"\n",
        "# Importing libraries \n",
        "print(\"Importing libraries\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "import datetime\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Start time \n",
        "print(\"Start time: \", datetime.datetime.now())\n",
        "\n",
        "# Project repo path function\n",
        "def ProjectRepo(path):\n",
        "   ProjectRepo = os.popen('bdvcli --get cluster.project_repo').read().rstrip()\n",
        "   return ProjectRepo + '/' + path\n",
        "\n",
        "# Reading in data \n",
        "print(\"Reading in data\")\n",
        "strpath= \"data/UCI_Income/\" + userID + \"/adult_train_cleaned.csv\"\n",
        "train = pd.read_csv(ProjectRepo(strpath))\n",
        "print(\"Done reading in data\")\n",
        "\n",
        "# Extracting target values \n",
        "y_train = train.pop('wage_class')\n",
        "train.pop('Unnamed: 0')\n",
        "\n",
        "# Model development / Training\n",
        "print(\"Training...\")\n",
        "xgdmat = xgb.DMatrix(train, y_train)\n",
        "our_params = {'eta': 0.1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'binary:logistic',\n",
        "              'max_depth': 3, 'min_child_weight': 1}\n",
        "cv_xgb = xgb.cv(params=our_params, dtrain=xgdmat, num_boost_round=3000, metrics=['error'],\n",
        "                early_stopping_rounds=100)\n",
        "optimal_rounds = len(cv_xgb)\n",
        "final_gb = xgb.train(our_params, xgdmat, num_boost_round = optimal_rounds)\n",
        "\n",
        "# Save model into project repo\n",
        "print(\"Saving model\")\n",
        "strpath= \"models/XGB_Income/\" + userID + \"/XGB.pickle.dat\"\n",
        "xgb.Booster.save_model(final_gb, ProjectRepo(strpath))\n",
        "\n",
        "# Finish time\n",
        "print(\"End time: \", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the unique log url and paste it into the cell below "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%logs --url http://hpecp-21.cplocal:10001/history/20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing Models\n",
        "Here we are going to test that the model prediction is as expected\n",
        "\n",
        "1. We're going to test the model that we've created here locally \n",
        "2. Then we will test the model that has been saved in the Project Repository \n",
        "3. Validate that the values are the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will take the first value in the adult_test_cleaned dataset\n",
        "Test the model by loading from Project Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strpath= \"data/UCI_Income/\" + userID + \"/adult_test_cleaned.csv\"\n",
        "cleaned = pd.read_csv(ProjectRepo(strpath))\n",
        "cleaned.tail(1)\n",
        "temp = cleaned.tail(1)\n",
        "y_test = temp.pop('wage_class')\n",
        "temp.set_index('age')\n",
        "temp.pop('Unnamed: 0')\n",
        "mat = xgb.DMatrix(temp) \n",
        "y_pred = final_gb.predict(mat)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = xgb.Booster({'nthread':325})\n",
        "strpath= \"models/XGB_Income/\" + userID + \"/XGB.pickle.dat\"\n",
        "model.load_model(ProjectRepo(strpath))\n",
        "temp = cleaned.tail(1)\n",
        "y_test = temp.pop('wage_class')\n",
        "temp.set_index('age')\n",
        "temp.pop('Unnamed: 0')\n",
        "mat = xgb.DMatrix(temp) \n",
        "y_pred = model.predict(mat)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validate that the 2 numbers are the same "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
